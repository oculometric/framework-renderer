## Introduction

![[UML.drawio]]
The artefact supports point, spot, and directional lights (up to 8 simultaneously) and supports shadow maps for spot and directional lights. Meshes are shaded with a PBR shader which includes albedo and normal mapping, and the artefact makes use of both solid and wireframe rasterisers and both triangle and line assembler modes. The artefact also makes use of the full-screen-quad technique to implement post-processing, including support for binding multiple render passes (colour, normal, depth) to the post-processing shader. The artefact also allows for resizing of the window/viewport. During drawing, scene objects are sorted according to the shader used in order to minimise the number of required context switches. When debug view is enabled, the renderer also draws object axes and bounding boxes. These features are implemented via the `FGraphicsEngine` class.

The post-processing shader includes a demo of a sharpening filter, and a sophisticated ASCII-art shader inspired by **INSERT ACEROLA LINK HERE**. It also features an implementation of depth-based fog and a skybox.

The artefact is built around a scene graph model, where a collection of objects (empty, mesh, light, and camera are all supported) are organised hierarchically by the use of per-object transforms, similar to **INSERT UNITY LINK HERE**. These transforms can have parents and children, and can be transformed (translation, rotation, scaling) in both local and world space, implemented by the `FTransform` class. Objects are managed in a scene by the `FScene` class, which provides functionality (start and update) which can be overridden by subclasses to create custom scenes (see `SurrealDemoScene`, `MyScene`, etc).

Scenes may be stored on disk in JSON format, which is deserialized by static functions. Required assets are loaded as needed. Materials may also be configured via JSON, including the ability to store values to be passed to individual shader uniforms. These parameters are stored within an `FMaterial` instance. JSON files are parsed by a custom parsing algorithm, which includes support for block and single-line comments.

Loaded assets (meshes, textures, shaders) are managed by the `FResourceManager` class, which prevents duplication and handles unloading assets when the program finishes. The artefact contains a custom OBJ file loader, including the ability to load texture coordinates and compute tangents at runtime, implemented by the `FMesh` and `FMeshData` classes.

**(INCOMPLETE) frustrum culling based on object bounding boxes**
**(INCOMPLETE) ambient occlusion using compute shaders**

One feature which was implemented successfully was post-processing support. This followed a standard industry technique where the scene is rendered to an intermediate buffer (rather than one of the framebuffers) which is then bound as a shader resource to be drawn on a single quad which fills the screen, giving the opportunity to use the shading language, along with data sampled from the intermediate screen buffer (or multiple) to produce a variety of interesting and stylistic effects. The artefact closely follows this implementation **LINK TO MICROSOFT DOCUMENTATION**, including the use of multiple different render passes, representing different aspects of the rendered scene: colour, normal, and depth buffers are exposed to the post-processing shader. The post-processing shader showcases an interesting stylised ASCII-art effect, as well as a sharpen filter. It is also used as the opportunity to draw fog and a skybox based on the values in the depth buffer, eliminating the need to draw per-object fog or have a separate skybox object. However, in its current state, the post-processing shader does not demonstrate usage of the normal buffer, which is something that could be improved. Additionally, an optimisation which has not been made is that post processing can be drawn using only a single triangle (instead of the two for a quad) which can be positioned to overfill the screen **LINK TO REFERENCE FOR THIS**. This could be implemented to improve the performance of the post-processing shader, but timing statistics show that the current post-processing shader has an extremely trivial performance cost (compared with drawing meshes).

Another feature the artefact showcases is normal mapping. This is a somewhat advanced technique which makes use of an additional texture (a normal map, or something a bump map though this requires additional computation to translate into a normal map) during shading to add the impression of greater surface detail, without adding geometry. This is done by **LINK TO LEARNOPENGL** using tangents and bitangents (vectors perpendicular to one another and to the surface normal), which represent the direction of the U and V texture coordinate axes in 3D space, to perturb the original surface normal according the normal map texture. The texture defines how to weight the sum of the normal, tangent, and bitangent vectors to produce the new surface normal. The artefact implements this by way of a T-B-N (tangent-bitangent-normal) matrix to transform the normal map colour value (in tangent space) into a 3D vector (in world space). This new normal is then used for lighting calculations instead of the static normal given by the vertex data. Implementing this technique requires the provision of tangents, which are calculated by the mesh reading code at load time following this algorithm **LINK TO WHATEVER ALGORITHM**. This technique provides excellent surface detail and improves realism, however the requirement to calculate mesh tangents when reading OBJ files noticeably increases the time required to load large meshes.

The artefact also implements shadow mapping for spot and directional lights. This technique takes advantage of the depth-buffering solution to the visibility problem (the fundamental geometric problem which rendering involves) to resolve shadows without expensive raytracing; the scene is rendered from the perspective of each light, treating the light as a camera, and the resulting depth buffer is stored in a texture. Later, when individual objects are rendered, this texture can be sampled, and the depth value compared with the depth of the current geometry relative to the light. If the depth value of the geometry is greater than the value in the texture, then the geometry must be in shadow **REFERENCE FOR THIS**. The artefact implements this technique by rendering all objects with a simple dedicated shader, without binding a colour buffer to avoid wasted computation. The artefact implements support for up to 8 lights total, though this number can be increased very easily. However, there are two limitations to the current implementation. The first of these is that spot lights are not supported; implementing support for these would require rendering the scene as a depth-cube-map from the light's position. The second limitation is that directional lights only render shadows in a fixed area around themselves, meaning that they must be positioned according to where the user wishes the shadow to have an effect. Modern graphics engines solve this by using multiple scaled tiers of shadow maps (cascades) for directional lights, and adjusting the position of these cascades according to the camera **REFERENCE**, but there was not enough time to implement such a feature.

**PARAGRAPH ABOUT SOMETHING WHICH I DIDNT FINISH/IMPLEMENT**

**FILL IN REFERENCES + BIBLIOGRAPHY**

**ALSO FINISH THE UML DIAGRAM**