## Introduction

![[UML.drawio]]
The artefact supports point, spot, and directional lights (up to 8 simultaneously) and supports shadow maps for spot and directional lights. Meshes are shaded with a PBR shader which includes albedo and normal mapping, and the artefact makes use of both solid and wireframe rasterisers and both triangle and line assembler modes. The artefact also makes use of the full-screen-quad technique to implement post-processing, including support for binding multiple render passes (colour, normal, depth) to the post-processing shader. The artefact also allows for resizing of the window/viewport. During drawing, scene objects are sorted according to the shader used in order to minimise the number of required context switches. When debug view is enabled, the renderer also draws object axes and bounding boxes. These features are implemented via the `FGraphicsEngine` class.

The post-processing shader includes a demo of a sharpening filter, and a sophisticated ASCII-art shader inspired by a YouTube video by Acerola (Gunnell (2024))^[7]. It also features an implementation of depth-based fog and a skybox.

The artefact is built around a scene graph model, where a collection of objects (empty, mesh, light, and camera are all supported) are organised hierarchically by the use of per-object transforms, similar to the `Transform` class provided by the Unity game engine (Unity (2024))^[6]. These transforms can have parents and children, and can be transformed (translation, rotation, scaling) in both local and world space, implemented by the `FTransform` class. Objects are managed in a scene by the `FScene` class, which provides functionality (start and update) which can be overridden by subclasses to create custom scenes (see `SurrealDemoScene`, `MyScene`, etc).

Scenes may be stored on disk in JSON format, which is deserialized by static functions. Required assets are loaded as needed. Materials may also be configured via JSON, including the ability to store values to be passed to individual shader uniforms. These parameters are stored within an `FMaterial` instance. JSON files are parsed by a custom parsing algorithm, which includes support for block and single-line comments.

Loaded assets (meshes, textures, shaders) are managed by the `FResourceManager` class, which prevents duplication and handles unloading assets when the program finishes. The artefact contains a custom OBJ file loader, including the ability to load texture coordinates and compute tangents at runtime, implemented by the `FMesh` and `FMeshData` classes.

**(INCOMPLETE) frustrum culling based on object bounding boxes**
**(INCOMPLETE) ambient occlusion using compute shaders**

One feature which was implemented successfully was post-processing support. This followed a standard industry technique where the scene is rendered to an intermediate buffer (rather than one of the framebuffers) which is then bound as a shader resource to be drawn on a single quad which fills the screen, giving the opportunity to use the shading language, along with data sampled from the intermediate screen buffer (or multiple) to produce a variety of interesting and stylistic effects (Magdics, et al. (2013))^[3]. The artefact closely follows this implementation, including the use of multiple different render passes, representing different aspects of the rendered scene: colour, normal, and depth buffers are exposed to the post-processing shader. The post-processing shader showcases an interesting stylised ASCII-art effect, as well as a sharpen filter. It is also used as the opportunity to draw fog and a skybox based on the values in the depth buffer, eliminating the need to draw per-object fog or have a separate skybox object. However, in its current state, the post-processing shader does not demonstrate usage of the normal buffer, which is something that could be improved. Additionally, an optimisation which has not been made is that post processing can be drawn using only a single triangle (instead of the two for a quad) which can be positioned to overfill the screen. This could be implemented to improve the performance of the post-processing shader, but timing statistics show that the current post-processing shader has an extremely trivial performance cost (compared with drawing meshes).

Another feature the artefact showcases is normal mapping. This is a somewhat advanced technique which makes use of an additional texture (a normal map, or something a bump map though this requires additional computation to translate into a normal map) during shading to add the impression of greater surface detail, without adding geometry. This is done by using tangents and bitangents (vectors perpendicular to one another and to the surface normal), which represent the direction of the U and V texture coordinate axes in 3D space, to perturb the original surface normal according the normal map texture (de Vries, (2013))^[2]. The texture defines how to weight the sum of the normal, tangent, and bitangent vectors to produce the new surface normal. The artefact implements this by way of a T-B-N (tangent-bitangent-normal) matrix to transform the normal map colour value (in tangent space) into a 3D vector (in world space). This new normal is then used for lighting calculations instead of the static normal given by the vertex data. Implementing this technique requires the provision of tangents, which are calculated by the mesh reading code at load time according to the algorithm described by Lengyel (2001)^[8]. This technique provides excellent surface detail and improves realism, however the requirement to calculate mesh tangents when reading OBJ files noticeably increases the time required to load large meshes.

The artefact also implements shadow mapping for spot and directional lights. This technique takes advantage of the depth-buffering solution to the visibility problem (the fundamental geometric problem which rendering involves) to resolve shadows without expensive raytracing; the scene is rendered from the perspective of each light, treating the light as a camera, and the resulting depth buffer is stored in a texture. This technique and it's advantages are described by Everitt, Rege, and Cebenoyan (2001)^[1]. Later, when individual objects are rendered, this texture can be sampled, and the depth value compared with the depth of the current geometry relative to the light. If the depth value of the geometry is greater than the value in the texture, then the geometry must be in shadow. The artefact implements this technique by rendering all objects with a simple dedicated shader, without binding a colour buffer to avoid wasted computation. The artefact implements support for up to 8 lights total, though this number can be increased very easily. However, there are two limitations to the current implementation. The first of these is that spot lights are not supported; implementing support for these would require rendering the scene as a depth-cube-map from the light's position. The second limitation is that directional lights only render shadows in a fixed area around themselves, meaning that they must be positioned according to where the user wishes the shadow to have an effect. Dimitrov (2007)^[4] presents a way to solve this by using multiple scaled tiers of shadow maps (cascades) for directional lights, and adjusting the position of these cascades according to the camera to improve quality, but there was not enough time to implement such a feature.

One element of the framework which was never fully implemented was frustrum culling. I tried to develop an algorithm which would test the object's axis-aligned bounding-box against the view frustrum. Considering some test cases, I devised a solution where, after the bounding box corners are transformed into clip space, they can be trivially checked against the clip space bounds (if any AABB corners are within the clip space cube, then the object must be drawn). However, this solution missed several cases, for instance where the entire view frustrum was contained within the AABB, or if the AABB was very narrow and intersected across the middle of the frustrum without having contained corners. Despite adding additional checks intended to catch these edge cases, there still remain some scenarios where objects are incorrectly culled, and the frustrum culling feature is disabled in the current version of the project. In order to complete this implementation, it would be ideal to find a source paper describing a proven algorithm, such as the one presented by Sunar, Zin, and Sembok (2008)^[5].

[1]: Everitt, C., Rege, A., and Cebenoyan, C. (2001) 'Hardware shadow mapping', _White paper, nVIDIA_, _2_.
[2]: de Vries, J. (n.d.). (2013) _LearnOpenGL - Normal Mapping_. learnopengl.com. Available at: https://learnopengl.com/Advanced-Lighting/Normal-Mapping.
[3]: Magdics, M. _et al._ (2013) ‘Post-processing NPR effects for video games’, _Proceedings of the 12th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and Its Applications in Industry_, pp. 147–156. doi:10.1145/2534329.2534348.
[4]: Dimitrov, R. (2007) 'Cascaded shadow maps', _Developer Documentation, NVIDIA Corp_.
[5]: Sunar, M.S., Zin, A.M., and Sembok, T.M. (2008) 'Improved View Frustum Culling Technique for Real-Time Virtual Heritage Application', _Int. J. Virtual Real._, _7_(3), pp.43-48.
[6]: Technologies, Unity. (2024) 'Transform', _Unity Documentation_. Available at: https://docs.unity3d.com/ScriptReference/Transform.html (Accessed: 11 November 2024).
[7]: Gunnell, G. (2024) 'I Tried Turning Games Into Text', _YouTube_. Available at: https://www.youtube.com/watch?v=gg40RWiaHRY (Accessed: 11 November 2024).
[8]: Lengyel, E. (2001) _Mathematics for 3D game programming and Computer Graphics, first edition_. Course Technology PTR.

**ALSO FINISH THE UML DIAGRAM**
**WRITE SOME MORE**